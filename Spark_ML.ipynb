{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ALPER KOCABIYIK **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "\n",
    "SPARK_HOME = \"\"\"/Users/gulnur/spark-2.1.0-bin-hadoop2.7\"\"\" #CHANGE THIS PATH TO YOURS!\n",
    "\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-0.10.4-src.zip\")) #BEWARE WITH py4j version!!\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"pyspark.zip\"))\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark.stop() \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"wind\") \\\n",
    "    .config(\"spark.sql.caseSensitive\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "  \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "import matplotlib.pyplot as plt \n",
    "# For plotting data\n",
    "import numpy as np              \n",
    "import os\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wind_sd = spark.read.csv(path=\"wind.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wind_sd.printSchema()\n",
    "#wind_sd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partition before repartition: 4\n",
      "Number of partition after repartition: 4\n"
     ]
    }
   ],
   "source": [
    "# repartition distributes the dataframe into x partitions.\n",
    "print(\"Number of partition before repartition: {}\".format(wind_sd.rdd.getNumPartitions()))\n",
    "wind_sd=wind_sd.repartition(4)\n",
    "print(\"Number of partition after repartition: {}\".format(wind_sd.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dividing data for train, test and validation data sets **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2528\n",
      "1299\n",
      "3827\n",
      "2110\n"
     ]
    }
   ],
   "source": [
    "Xtrain_sd = wind_sd.filter(\"year<=2006\")\n",
    "Xval_sd = wind_sd.filter((wind_sd['year']==2007) | (wind_sd['year']==2008))\n",
    "XtrainVal_sd = Xtrain_sd.union(Xval_sd)\n",
    "Xtest_sd = wind_sd.filter(\"year>=2009\")\n",
    "print Xtrain_sd.count()\n",
    "print Xval_sd.count()\n",
    "print XtrainVal_sd.count()\n",
    "print Xtest_sd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "ignore = ['energy', 'steps', 'year', 'month', 'day', 'hour']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in Xtrain_sd.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "Xtrain_sd = assembler.transform(Xtrain_sd).select(['energy', 'features'])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in Xval_sd.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "Xval_sd = assembler.transform(Xval_sd).select(['energy', 'features'])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in XtrainVal_sd.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "XtrainVal_sd = assembler.transform(XtrainVal_sd).select(['energy', 'features'])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in Xtest_sd.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "Xtest_sd = assembler.transform(Xtest_sd).select(['energy', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# change the column name 'energy' to 'label'\n",
    "Xtrain_sd = Xtrain_sd.selectExpr(\"energy as label\", \"features as features\")\n",
    "Xval_sd = Xval_sd.selectExpr(\"energy as label\", \"features as features\")\n",
    "XtrainVal_sd = XtrainVal_sd.selectExpr(\"energy as label\", \"features as features\")\n",
    "Xtest_sd = Xtest_sd.selectExpr(\"energy as label\", \"features as features\")\n",
    "\n",
    "print(Xtrain_sd.printSchema())\n",
    "print(Xval_sd.printSchema())\n",
    "print(XtrainVal_sd.printSchema())\n",
    "print(Xtest_sd.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Baseline results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train a decision tree with train+validation datasets (XtrainVal_sd) and evaluate it\n",
    "with the test dataset. The performance measure is MAE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor()\n",
    "model=dt.fit(XtrainVal_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions are computed on the test set.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------------------+------------------+\n",
      "|  label|            features|PCA_4cf38b628ce512accb1c__output|        prediction|\n",
      "+-------+--------------------+--------------------------------+------------------+\n",
      "|1191.99|[2450130.01534,24...|            [4.66284887452031...| 410.4751086956523|\n",
      "| 916.83|[2496225.43889,24...|            [1.50038343889636...|1408.7215853658538|\n",
      "|    2.9|[2514298.53184,25...|            [3766248.55510174...| 304.9938283828383|\n",
      "|  73.69|[2499306.90124,24...|            [1.14180887567692...|  945.900981308411|\n",
      "|1276.97|[2468415.46712,24...|            [6698163.84459003...|  311.455965665236|\n",
      "| 786.37|[2435648.94961,24...|            [4901080.40461038...|1108.1148979591835|\n",
      "| 151.61|[2454996.19561,24...|            [8183811.76678010...|  630.459054054054|\n",
      "|1351.63|[2465849.08793,24...|            [6284468.07125426...|  630.459054054054|\n",
      "| 943.94|[2462993.53924,24...|            [1564626.61208993...|124.55350364963505|\n",
      "| 221.58|[2476114.60472,24...|            [1096955.70571803...|  311.455965665236|\n",
      "| 243.69|[2491458.66063,24...|            [4929835.91518329...|238.12784883720937|\n",
      "| 283.92|[2499388.23016,24...|            [3125880.22548184...|  311.455965665236|\n",
      "|  510.5|[2477998.72466,24...|            [6227099.72513891...|  630.459054054054|\n",
      "| 226.01|[2442250.14681,24...|            [3761244.14521344...| 756.2209621993128|\n",
      "| 756.53|[2480768.42615,24...|            [4215502.99277007...| 756.2209621993128|\n",
      "| 116.69|[2476670.35233,24...|            [3883113.84696684...|  311.455965665236|\n",
      "| 533.03|[2427136.52284,24...|            [5388630.99853748...| 756.2209621993128|\n",
      "| 180.57|[2443176.39282,24...|            [1.16544141568128...| 410.4751086956523|\n",
      "| 522.53|[2468795.00208,24...|            [2.11925101472123...| 410.4751086956523|\n",
      "|  62.31|[2479046.96405,24...|            [3780903.24912602...| 304.9938283828383|\n",
      "+-------+--------------------+--------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Error for Decision Tree model : 482\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "predictions_sd = model.transform(Xtest_sd)\n",
    "predictions_sd.show()\n",
    "\n",
    "# check error\n",
    "evaluator = RegressionEvaluator(\n",
    "labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(predictions_sd)\n",
    "print \"Error for Decision Tree model : %d\"  %mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/3. For different k values (k = number of PCA components to use)\n",
    "### a. Compute k PCA components from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Pipeline.getStages of Pipeline_433d946954114661d510>\n",
      "<bound method Pipeline.getStages of Pipeline_4335b5a20abee503be3c>\n",
      "<bound method Pipeline.getStages of Pipeline_4ef5955999360e13999c>\n",
      "<bound method Pipeline.getStages of Pipeline_4cc8a81d098449a7f5e1>\n",
      "<bound method Pipeline.getStages of Pipeline_413c82b186478320c72b>\n",
      "<bound method Pipeline.getStages of Pipeline_4e06942dc77e28d7b3fd>\n",
      "<bound method Pipeline.getStages of Pipeline_4a57a94b9e6a5afe08b8>\n",
      "<bound method Pipeline.getStages of Pipeline_4d6e8d3646e586e63189>\n",
      "<bound method Pipeline.getStages of Pipeline_41f0a5dda2b93f8902f8>\n",
      "<bound method Pipeline.getStages of Pipeline_4fd19162672fad07b45e>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Pipeline components from 10 to '10*limit' => decision tree \n",
    "pllist = []\n",
    "limit = 10\n",
    "for kval in range(10,limit*10+1 ,10):\n",
    "    pca = PCA(k=kval, inputCol=\"features\")\n",
    "    dt = DecisionTreeRegressor(featuresCol=pca.getOutputCol(), labelCol=\"label\")\n",
    "    pl = Pipeline(stages=[pca, dt])\n",
    "    pllist.append(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Train a decision tree with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modellist =[]\n",
    "for i in range(0,limit,1):\n",
    "    model = pllist[i].fit(Xtrain_sd)\n",
    "    modellist.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Evaluate the decision tree on the validation set. MAE will be used as performance measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for PCA model with 10 principal components: 470\n",
      "Error for PCA model with 20 principal components: 474\n",
      "Error for PCA model with 30 principal components: 474\n",
      "Error for PCA model with 40 principal components: 447\n",
      "Error for PCA model with 50 principal components: 448\n",
      "Error for PCA model with 60 principal components: 448\n",
      "Error for PCA model with 70 principal components: 445\n",
      "Error for PCA model with 80 principal components: 446\n",
      "Error for PCA model with 90 principal components: 449\n",
      "Error for PCA model with 100 principal components: 444\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "predictions = []\n",
    "mae = []\n",
    "for i in range(0,limit,1):\n",
    "    # test model\n",
    "    predictions.append(modellist[i].transform(Xval_sd))\n",
    "    #predictions[i].show()\n",
    "\n",
    "    # check error\n",
    "    mae.append(evaluator.evaluate(predictions[i]))\n",
    "    print \"Error for PCA model with %d principal components: %d\" %((i+1)*10, mae[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the final decisiontree, and evaluating it on the test set, using the best PCA result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for best PCA model with 100 components: 464\n"
     ]
    }
   ],
   "source": [
    "# Best PCA pipeline model\n",
    "# Training using train+validation dataset : XtrainVal_sd\n",
    "bestPCAindex = mae.index(min(mae))\n",
    "model_bestPCA = pllist[bestPCAindex].fit(XtrainVal_sd)\n",
    "\n",
    "# test model\n",
    "predictions_bestPCA = model_bestPCA.transform(Xtest_sd)\n",
    "#predictions_sd.show()\n",
    "\n",
    "# check error\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(predictions_bestPCA)\n",
    "print \"Error for best PCA model with %d components: %d\" %((bestPCAindex+1)*10, mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model MAE:482\n",
    "\n",
    "Best PCA model MAE:464\n",
    "\n",
    "We get a lower MAE with PCA, by using less number of attributes. That is, there is alot of redundancy in the input attributes that can be removed via PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
